{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate import AlignedSent, Alignment\n",
    "import goslate\n",
    "import time\n",
    "from translate import Translator\n",
    "from difflib import SequenceMatcher\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the MAG csv datasets\n",
    "DIR_ROOT = \"../data/mag/\"\n",
    "MAG_DICT = {}\n",
    "\n",
    "for r, d, f in os.walk(DIR_ROOT):\n",
    "    for file in f:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(r, file)\n",
    "            print(\"Reading mag file: \",file_path)\n",
    "            with open(file_path, 'r') as csv_file:\n",
    "                MAG_DICT[file] = pd.DataFrame(csv.reader(csv_file))         \n",
    "                #Convert the first line into columns\n",
    "                MAG_DICT[file].columns = MAG_DICT[file].iloc[0]\n",
    "                MAG_DICT[file] = MAG_DICT[file].reindex(MAG_DICT[file].index.drop(0)).reset_index(drop=True)\n",
    "                MAG_DICT[file].columns.name = None\n",
    "                \n",
    "                #Normalize the dataframe\n",
    "                def aff_val_filter(x):\n",
    "                    return x.strip() != \"\"\n",
    "                \n",
    "                MAG_DICT[file][\"AA.DAfN\"] = MAG_DICT[file][\"AA.DAfN\"].apply(lambda x: list(filter(aff_val_filter, x.split(\";;\")))) \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST the results\n",
    "# COMMENT this PArt\n",
    "#MAG_DICT[\"International Journal of Humanities and Arts Computing.csv\"]\n",
    "#MAG_DICT[\"Literary and Linguistics Computing part 2.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACDH_INST_PATH = \"../data/acdh/institutions.json\"\n",
    "with open(ACDH_INST_PATH) as json_file:\n",
    "    acdh_inst = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the unique institutions we have \n",
    "# Some values should ba manually corrected \n",
    "# For instance: 'Consiglio Nazionale delle Ricerche'  _and_ 'Consiglio Nazionale delle Ricerche - CNR' are the same institution\n",
    "# This goes out of this work\n",
    "\n",
    "inst_df = pd.DataFrame(acdh_inst)\n",
    "inst_df = inst_df[[\"name\",\"id\"]]\n",
    "inst_df.columns = ['acdh_name','id']\n",
    "inst_df['acdh_name'] = inst_df['acdh_name'].apply(lambda x: x.strip().lower()) \n",
    "#inst_df['language'] = inst_df['acdh_name'].apply(lambda x: gs.detect(x))\n",
    "inst_df['acdh_name_eng'] = inst_df['acdh_name'].apply(lambda x: Translator(from_lang = \"autodetect\", to_lang=\"en\").translate(x).lower())\n",
    "inst_df['acdh_name_eng'] = inst_df['acdh_name_eng'].apply(lambda x: '' if 'please select' in x else x)\n",
    "#inst_df['acdh_tokenized'] = inst_df['acdh_name'].apply(lambda x: list(set(word_tokenize(x)) - set(stopwords.words('english'))) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we check how many elements are similar to what we have in MAG\n",
    "ROOT_PATH = '../data/mag_acdh_join/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "for k in MAG_DICT:\n",
    "    #if already processed move on the next one\n",
    "    if os.path.exists('../data/mag_acdh_join/'+k.replace(\".csv\",\"\")+'.json'):\n",
    "        continue\n",
    "    print(\"Join \"+k+\"with acdh institutions ...\")\n",
    "    new_column = []\n",
    "    for index, row in MAG_DICT[k].iterrows():\n",
    "        \n",
    "        l_aff = []\n",
    "        for aff in row['AA.DAfN']:    \n",
    "            # Check inside the ACDH affiliations if present\n",
    "            flag_found = -1\n",
    "            for index_acdh, row_acdh in inst_df.iterrows():\n",
    "                if ((similar(row_acdh[\"acdh_name\"],aff) > 0.8) or (similar(row_acdh[\"acdh_name_eng\"],aff) > 0.8)):\n",
    "                    flag_found = row_acdh[\"id\"]\n",
    "                    break\n",
    "            \n",
    "            l_aff.append(flag_found)\n",
    " \n",
    "        new_column.append(l_aff)\n",
    "\n",
    "    # add the new column\n",
    "    MAG_DICT[k][\"acdh_aff\"] = new_column\n",
    "    \n",
    "    MAG_DICT[k][[\"AA.DAfN\",\"DOI\",\"acdh_aff\"]].to_json(r'../data/mag_acdh_join/'+k.replace(\".csv\",\"\")+'.json',orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/res/affiliation.json\") as json_file:\n",
    "    affiliation_df = pd.DataFrame(json.load(json_file))\n",
    "\n",
    "ERRORS = [\" Pisa\"]\n",
    "affiliation_df = affiliation_df.set_index('DOI').drop(\" Pisa\")\n",
    "affiliation_df.to_json(\"../data/res/affiliation(with_id).json\", orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DRAFT\n",
    "\n",
    "#all_data = []\n",
    "#for r, d, f in os.walk(ROOT_PATH):\n",
    "#    for file in f:\n",
    "#        if file.endswith(\".json\"):\n",
    "#            file_path = os.path.join(ROOT_PATH, file)\n",
    "#            with open(file_path) as json_file:\n",
    "#                all_data = all_data + json.load(json_file)\n",
    "\n",
    "#with open(ROOT_PATH+'affiliation.json', 'w') as file:\n",
    "#     file.write(json.dumps(all_data)) # use `json.loads` to do the reverse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
